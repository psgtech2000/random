import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

# ===============================
# Helper functions
# ===============================

def simulate_attack(G, attack_type, fractions):
    """
    Simulate adaptive attacks on the network G.
    attack_type: 'degree' or 'clustering'
    fractions: list of fractions of nodes to remove
    Returns: list of giant component sizes
    """
    G_copy = G.copy()
    N = G_copy.number_of_nodes()
    giant_sizes = []

    for f in fractions:
        num_remove = int(f * N)
        if num_remove > 0:
            # Adaptive attack: recalculate metrics at each step
            if attack_type == 'degree':
                metric = dict(G_copy.degree())
            elif attack_type == 'clustering':
                metric = nx.clustering(G_copy)
            else:
                raise ValueError("attack_type must be 'degree' or 'clustering'")
            
            # Sort nodes descending by metric
            nodes_sorted = sorted(metric, key=metric.get, reverse=True)
            # Remove top nodes
            nodes_to_remove = nodes_sorted[:num_remove]
            G_copy.remove_nodes_from(nodes_to_remove)
        
        # Giant component size
        if len(G_copy) > 0:
            giant = max(nx.connected_components(G_copy), key=len)
            giant_sizes.append(len(giant)/N)
        else:
            giant_sizes.append(0)
    
    return giant_sizes


def plot_attack_results(fractions, sizes_deg, sizes_clu, title="Attack Results"):
    plt.figure(figsize=(8,5))
    plt.plot(fractions, sizes_deg, marker='o', label='Degree attack')
    plt.plot(fractions, sizes_clu, marker='s', label='Clustering attack')
    plt.xlabel("Fraction of nodes removed")
    plt.ylabel("Relative size of giant component")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()


# ===============================
# Parameters
# ===============================
N = 10000
fractions = np.linspace(0, 0.5, 11)

# ===============================
# 1. Configuration Model (Power-Law)
# ===============================
print("Generating configuration model (power-law, Î³=2.5) ...")
degrees = nx.utils.powerlaw_sequence(N, 2.5)
seq = [max(1, int(round(x))) for x in degrees]
if sum(seq) % 2 == 1:
    seq[np.random.randint(0, N)] += 1

G_config = nx.configuration_model(seq)
G_config = nx.Graph(G_config)
G_config.remove_edges_from(nx.selfloop_edges(G_config))

# Simulate attacks
sizes_deg = simulate_attack(G_config, 'degree', fractions)
sizes_clu = simulate_attack(G_config, 'clustering', fractions)

plot_attack_results(fractions, sizes_deg, sizes_clu, title="Config Model Attack")

# ===============================
# 2. Hierarchical Model
# ===============================
print("Generating hierarchical model ...")
G_hier = nx.powerlaw_cluster_graph(N, m=4, p=0.1)

sizes_deg_h = simulate_attack(G_hier, 'degree', fractions)
sizes_clu_h = simulate_attack(G_hier, 'clustering', fractions)

plot_attack_results(fractions, sizes_deg_h, sizes_clu_h, title="Hierarchical Model Attack")

# ===============================
# Analysis (to include in report)
# ===============================
print("\nAnalysis:")
print("- Degree-based attack fragments the network faster than clustering-based attack.")
print("- Protecting high-degree nodes limits damage the most.")
print("- If all topological information (degree, clustering) were secret, targeted attacks would be less effective.")